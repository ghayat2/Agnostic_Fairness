{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import *\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "from torch.utils import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    return pd.read_csv(filepath) \n",
    "\n",
    "def minmax_scale(df):\n",
    "    minmax_scale = preprocessing.MinMaxScaler().fit(df.values)\n",
    "    scaled_df = minmax_scale.transform(df.values)\n",
    "    scaled_df = pd.DataFrame(scaled_df, index=df.index, columns=df.columns)\n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "def top_k_proxy_features(df, label, protect, k):\n",
    "    correlations = []\n",
    "    for feature in df:\n",
    "        if feature not in [protect, label]:\n",
    "            correlation_score = normalized_mutual_info_score(df[feature], df[protect], average_method='arithmetic')\n",
    "            correlations.append((feature, correlation_score))\n",
    "    top_k = sorted(correlations, key=lambda kv:kv[1], reverse=True)[:k]\n",
    "    print(top_k, \" top_k\")\n",
    "    return top_k\n",
    "\n",
    "def df_without_k_proxies(df, label, protect, k):\n",
    "    top_k = top_k_proxy_features(df, protect, label, k)\n",
    "    top_k_features = set([feature for feature, _ in top_k])\n",
    "    remaining_features = set(df.columns) - top_k_features\n",
    "    return df[remaining_features]\n",
    "\n",
    "def balance_df_label(df, label, downsample=True):\n",
    "    majority_label, minority_label = df[label].value_counts().index[0], df[label].value_counts().index[1]\n",
    "    df_minority = df[df[label] == minority_label]\n",
    "    df_majority = df[df[label] == majority_label]\n",
    "    \n",
    "    if downsample:\n",
    "        df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=1)\n",
    "        df_label_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "    else:    \n",
    "        df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=1)\n",
    "        df_label_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "    return df_label_balanced\n",
    "\n",
    "def balance_df(df, label, protect, label_only=False, downsample=True):\n",
    "    if label_only:\n",
    "        df_balanced_label = balance_df_label(df, label, downsample=downsample)\n",
    "    else:\n",
    "        majority_label_class, minority_label_class = df[label].value_counts().index[0], df[label].value_counts().index[1]\n",
    "        majority_protect_class, minority_protect_class = df[protect].value_counts().index[0], df[protect].value_counts().index[1]\n",
    "\n",
    "        # balancing across demographics group for greater label\n",
    "        df_minority_greater = df[(df[label]==majority_label_class) & (df[protect]==minority_protect_class)]\n",
    "        df_majority_greater = df[(df[label]==majority_label_class) & (df[protect]==majority_protect_class)]\n",
    "        df_majority_downsampled_greater = resample(df_majority_greater, replace=False, n_samples=len(df_minority_greater), random_state=1)\n",
    "\n",
    "         # balancing across demographics group for smaller label \n",
    "        df_minority_smaller = df[(df[label]==minority_label_class) & (df[protect]==minority_protect_class)]\n",
    "        df_majority_smaller = df[(df[label]==minority_label_class) & (df[protect]==majority_protect_class)]\n",
    "        df_majority_downsampled_smaller = resample(df_majority_smaller, replace=False, n_samples=len(df_minority_smaller), random_state=1)\n",
    "\n",
    "        df_protect_balanced = pd.concat([df_majority_downsampled_greater, df_majority_downsampled_smaller, df_minority_greater, df_minority_smaller])\n",
    "        # balancing across labels for all demographic groups after balancing across demographic groups\n",
    "        \n",
    "        df_balanced_label = balance_df_label(df_protect_balanced, label, downsample=True) \n",
    "    conf = confusion_matrix(df_balanced_label[label].values, df_balanced_label[protect].values)\n",
    "    print(conf,\" balanced\", label, protect)\n",
    "    return df_balanced_label\n",
    "    \n",
    "def split_train_test(df, train=0.75):\n",
    "    np.random.seed(seed=1)\n",
    "    shuffled = np.random.permutation(df.index)\n",
    "    n_train = int(len(shuffled) * train)\n",
    "    i_train, i_test = shuffled[:n_train], shuffled[n_train:]\n",
    "    return df.loc[i_train], df.loc[i_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, df, label_column, protect_column):\n",
    "        'Initialization'\n",
    "        self.features = df.drop([label_column, protect_column], axis=1).values\n",
    "        self.label = df[label_column].values\n",
    "        self.protect = df[protect_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        X = self.features[index]\n",
    "        y = self.label[index]\n",
    "        z = self.protect[index]\n",
    "        return X, y, z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_dataset(filepath, label, protect, is_scaled=True, num_proxy_to_remove=0, balanced={\"label_only\":False,\"downsample\":True}):\n",
    "    # Loading the dataset\n",
    "    df = get_data(filepath)\n",
    "    print(\"loaded\")\n",
    "  \n",
    "    # Scaling the dataset\n",
    "    if is_scaled:\n",
    "        df = minmax_scale(df)\n",
    "        print(\"scaled\")\n",
    "        \n",
    "    # Removing proxy features\n",
    "    if num_proxy_to_remove > 0:\n",
    "        df = df_without_k_proxies(df, label, protect, num_proxy_to_remove)\n",
    "        print(\"proxy removed\")\n",
    "        \n",
    "    # Balancing the dataset\n",
    "    if balanced is not None:\n",
    "        df = balance_df(df, label, protect, label_only=balanced[\"label_only\"], downsample=balanced[\"downsample\"])\n",
    "        print(\"balanced\")\n",
    "        \n",
    "    # Splitting dataset into train, test features\n",
    "    train_df, test_df = split_train_test(df)\n",
    "    train_dataset = Dataset(train_df, label, protect)\n",
    "    test_dataset = Dataset(test_df, label, protect)\n",
    "    \n",
    "    return train_dataset, test_dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
