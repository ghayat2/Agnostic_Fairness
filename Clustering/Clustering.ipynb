{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacterial-integral",
   "metadata": {},
   "source": [
    "This notebook aims at clustering the dataset to expose potential bias. It uses a small convolutional network to find embeddings of every sample of the dataset, before applying PCA and using the k-means algorithm with a customized distance metric to yield a clustering of the dataset. This notebook can be ran sequencially, cell by cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-firewall",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys  \n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "\n",
    "import visdom\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity, pairwise_distances\n",
    "\n",
    "sys.path.insert(0, '../Resnet/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import *\n",
    "from my_ImageFolder import *\n",
    "from fairness_metrics import *\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-welding",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bask_r_f = '../Datasets/basket_volley/basket/basket_f_r/'\n",
    "path_bask_y_f = '../Datasets/basket_volley/basket/basket_f_y/'\n",
    "path_bask_r_m = '../Datasets/basket_volley/basket/basket_m_r/'\n",
    "path_bask_y_m = '../Datasets/basket_volley/basket/basket_m_y/'\n",
    "\n",
    "bask_r_f = os.listdir(path_bask_r_f)\n",
    "bask_y_f = os.listdir(path_bask_y_f)\n",
    "bask_r_m = os.listdir(path_bask_r_m)\n",
    "bask_y_m = os.listdir(path_bask_y_m)\n",
    "\n",
    "path_voll_r_f = '../Datasets/basket_volley/volley/volley_f_r/'\n",
    "path_voll_y_f = '../Datasets/basket_volley/volley/volley_f_y/'\n",
    "path_voll_r_m = '../Datasets/basket_volley/volley/volley_m_r/'\n",
    "path_voll_y_m = '../Datasets/basket_volley/volley/volley_m_y/'\n",
    "\n",
    "voll_r_f = os.listdir(path_voll_r_f)\n",
    "voll_y_f = os.listdir(path_voll_y_f)\n",
    "voll_r_m = os.listdir(path_voll_r_m)\n",
    "voll_y_m = os.listdir(path_voll_y_m)\n",
    "\n",
    "class0_min, class1_min = bask_y_m + bask_y_f,voll_r_m + voll_r_f\n",
    "protected_groups = set(class0_min + class1_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-seven",
   "metadata": {},
   "source": [
    "# Defining the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_PROTECTED, BIAS, VAL_MODE, START_EPOCH, NUM_EPOCH, SHOW_PROGRESS, ID, DATASET, NUM_TRIALS, BIAS = 1, 0.8, False, 0, 3, False, 0, \"basket_volley\", 1, 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '../Datasets/basket_volley/train_test_split'\n",
    "image_datasets = {\n",
    "    x: my_ImageFolder(os.path.join(data_dir, f\"train_{BIAS}\" if x == \"train\" else x), data_transforms[x],\n",
    "                      protected_groups, W_PROTECTED)\n",
    "    for x in ['train', 'test']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-flooring",
   "metadata": {},
   "source": [
    "# Defining convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 1, 4)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 12, 5)\n",
    "        # self.fc1 = nn.Linear(33708, 2048)\n",
    "        self.fc2 = nn.Linear(12100, 512)\n",
    "        self.fc3 = nn.Linear(512, len(class_names))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.fc2.register_forward_hook(get_activation('fc2'))\n",
    "\n",
    "criterion = weighted_cross_entropy_loss # nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-vacuum",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-shock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = train_model(net, criterion, optimizer, scheduler, dataloaders, dataset_sizes, device,\n",
    "                             start_epoch=START_EPOCH,\n",
    "                             num_epochs=NUM_EPOCH,\n",
    "                             val_mode=VAL_MODE, show_progress=SHOW_PROGRESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-boulder",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Acc. on Training set: {float(accuracy(net, device, dataloaders['train']))}\")\n",
    "print(f\"Acc. on Test set: {float(accuracy(net, device, dataloaders['test']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-stylus",
   "metadata": {},
   "source": [
    "# Extracting represenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform_0, X_transform_1 = np.array([[]]).reshape(0, net.fc2.out_features),np.array([[]]).reshape(0, net.fc2.out_features)\n",
    "indexes_0, indexes_1 = np.array([]).astype(int), np.array([]).astype(int)\n",
    "\n",
    "for i, (inputs, labels, clusters, index) in enumerate(dataloaders[\"train\"]):\n",
    "    output = net(inputs)\n",
    "    output = activation['fc2']\n",
    "    for j, l in enumerate(labels):\n",
    "        if not l:\n",
    "            X_transform_0 = np.concatenate([X_transform_0, output[j].numpy().reshape((1, -1))])\n",
    "            indexes_0 = np.concatenate([indexes_0, index[j].numpy().reshape(-1)])\n",
    "        else:\n",
    "            X_transform_1 = np.concatenate([X_transform_1, output[j].numpy().reshape((1, -1))])\n",
    "            indexes_1 = np.concatenate([indexes_1, index[j].numpy().reshape(-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-envelope",
   "metadata": {},
   "source": [
    "# PCA reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_0 = PCA(n_components=10)\n",
    "pca_1 = PCA(n_components=10)\n",
    "\n",
    "X_reducted_0 = pca_0.fit_transform(X_transform_0)\n",
    "X_reducted_1 = pca_1.fit_transform(X_transform_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues_0 = pca_0.explained_variance_\n",
    "eigenvalues_1 = pca_1.explained_variance_\n",
    "eigenvalues_0, eigenvalues_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-trustee",
   "metadata": {},
   "source": [
    "# k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_distance_0(v1, v2):\n",
    "    dist = 0\n",
    "    for i, (c1, c2) in enumerate(zip(v1, v2)):\n",
    "        dist += eigenvalues_0[i] * float(abs(c1 - c2))\n",
    "    return dist\n",
    "\n",
    "def my_distance_1(v1, v2):\n",
    "    dist = 0\n",
    "    for i, (c1, c2) in enumerate(zip(v1, v2)):\n",
    "        dist += eigenvalues_1[i] * float(abs(c1 - c2))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_0 = KMeansClusterer(2, distance=my_distance_0, repeats=25) # nltk.cluster.util.cosine_distance\n",
    "km_1 = KMeansClusterer(2, distance=my_distance_1, repeats=25) # nltk.cluster.util.cosine_distance\n",
    "\n",
    "kmeans_0 = km_0.cluster(X_reducted_0, assign_clusters=True)\n",
    "kmeans_1 = km_1.cluster(X_reducted_1, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-wells",
   "metadata": {},
   "source": [
    "# Create cluster folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_paths_0 = view_clusters(\"class_0/\", kmeans_0, indexes_0)\n",
    "cluster_paths_1 = view_clusters(\"class_1/\", kmeans_1, indexes_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-mistake",
   "metadata": {},
   "source": [
    "# Performance statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-baghdad",
   "metadata": {},
   "source": [
    "### Basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Kmeans with custom distance metric - this make the color attribute even more important\n",
    "statistics(\"class_0/\", cluster_paths_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-standing",
   "metadata": {},
   "source": [
    "### Volley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Kmeans with cosine_distance\n",
    "statistics(\"class_1/\", cluster_paths_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-filling",
   "metadata": {},
   "source": [
    "# Helper methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_closest(list, top_k=5):\n",
    "    for i in range(top_k):\n",
    "        img_show(list[i][1])\n",
    "        print(list[i][0])\n",
    "        \n",
    "def img_show(i):\n",
    "    # clear_output(wait=True)\n",
    "    image = Image.open(image_datasets[\"train\"].samples[i][0], \"r\")\n",
    "    plt.imshow(np.asarray(image))\n",
    "    plt.show()\n",
    "\n",
    "def view_clusters(path, kmeans, indexes):\n",
    "    K = len(set(kmeans))\n",
    "    \n",
    "    paths = []\n",
    "    for k in range(K):\n",
    "        paths.append(os.path.join(path, f\"clustering_{K}/cluster_{k}\"))\n",
    "        os.makedirs(paths[-1], exist_ok=True)\n",
    "        \n",
    "    for i in range(len(kmeans)):\n",
    "        src = image_datasets[\"train\"].samples[indexes[i]][0]\n",
    "        dst = os.path.join(path, f\"clustering_{K}/cluster_{kmeans[i]}/\") + src.split(\"/\")[-1]\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def list_clusters(path, kmeans, indexes):\n",
    "    K = len(set(kmeans))\n",
    "    bask, voll = os.listdir(os.path.join(path, \"basket\")), os.listdir(os.path.join(path, \"volley\"))\n",
    "    \n",
    "    lists = [[] for _ in range(K)]\n",
    "    for i in range(len(kmeans)):\n",
    "        src = bask[indexes[i]] if indexes[i] < len(bask) else voll[indexes[i] - len(bask)]\n",
    "        lists[kmeans[i]].append(src)\n",
    "        \n",
    "    return lists\n",
    "\n",
    "def proximity(pca, km, X_reducted, indexes, kmeans):\n",
    "    proj = pca.inverse_transform(km.cluster_centers_)\n",
    "    list_0, list_1 = [], []\n",
    "    for rep, id, c in zip(X_reducted, indexes, kmeans):\n",
    "        dist = mse(rep, km.cluster_centers_[0]) if not c else mse(rep, km.cluster_centers_[1])\n",
    "        list_1.append((dist, id)) if c else list_0.append((dist, id))      \n",
    "    list_0.sort(), list_1.sort()\n",
    "    return list_0, list_1\n",
    "\n",
    "def closest_to(emb, X_transform, indexes, metric=cosine_similarity, descending=False):\n",
    "    l = []\n",
    "    for i, (emb2, idx) in enumerate(zip(X_transform, indexes)):\n",
    "        dist = metric(emb.reshape((1, -1)), emb2.reshape((1, -1)))\n",
    "        l.append((dist, idx))\n",
    "    l.sort(reverse=descending)\n",
    "    return l\n",
    "\n",
    "def statistics(path, clusters):\n",
    "    K = len(set(clusters))\n",
    "    \n",
    "    for k in range(K):\n",
    "        n_bask, n_voll, n_r, n_y, n_m, n_f = 0, 0, 0, 0, 0, 0\n",
    "        cluster = os.listdir(clusters[k])\n",
    "        for img in cluster:\n",
    "            if img in bask_r_f:\n",
    "                n_bask += 1\n",
    "                n_f += 1\n",
    "                n_r += 1\n",
    "                \n",
    "            if img in bask_r_m:\n",
    "                n_bask += 1\n",
    "                n_m += 1\n",
    "                n_r += 1\n",
    "                \n",
    "            if img in bask_y_f:\n",
    "                n_bask += 1\n",
    "                n_f += 1\n",
    "                n_y += 1\n",
    "            \n",
    "            if img in bask_y_m:\n",
    "                n_bask += 1\n",
    "                n_m += 1\n",
    "                n_y += 1\n",
    "            \n",
    "            if img in voll_r_f:\n",
    "                n_voll += 1\n",
    "                n_f += 1\n",
    "                n_r += 1\n",
    "            \n",
    "            if img in voll_r_m:\n",
    "                n_voll += 1\n",
    "                n_m += 1\n",
    "                n_r += 1\n",
    "                \n",
    "            if img in voll_y_f:\n",
    "                n_voll += 1\n",
    "                n_f += 1\n",
    "                n_y += 1\n",
    "            \n",
    "            if img in voll_y_m:\n",
    "                n_voll += 1\n",
    "                n_m += 1\n",
    "                n_y += 1\n",
    "                \n",
    "        \n",
    "        print(f\"--------------Cluster {k}--------- \\n n. samples: {len(cluster)}\\n n. of bask: {n_bask} ({n_bask/len(cluster)*100:.1f}%)\\n n. of volley: {n_voll} ({n_voll/len(cluster)*100:.1f}%)\\n n. of red: {n_r} ({n_r/len(cluster)*100:.1f}%)\\n n. of yellow: {n_y} ({n_y/len(cluster)*100:.1f}%)\\n n. of males: {n_m} ({n_m/len(cluster)*100:.1f}%)\\n n. of females: {n_f} ({n_f/len(cluster)*100:.1f}%)\")\n",
    "        \n",
    "def statistics_colors(clusters, K=2):\n",
    "    stats = []\n",
    "    for k in range(K):\n",
    "        n_r, n_y = 0, 0\n",
    "        cluster = clusters[k]\n",
    "        for img in cluster:\n",
    "            if img in bask_r_f or img in bask_r_m or img in voll_r_f or img in voll_r_m:\n",
    "                n_r += 1\n",
    "            else:\n",
    "                n_y += 1\n",
    "        stats.append(n_r/(n_r+n_y))\n",
    "    return stats\n",
    "\n",
    "def make_save_dict(samples, k_means_list, indexes_list, save=False, name=\"dict.txt\"):\n",
    "    dic = {}\n",
    "    for k_means, indexes in zip(k_means_list, indexes_list):\n",
    "        for cluster, idx in zip(k_means, indexes):\n",
    "            img = samples[idx][0].split(\"/\")[-1]\n",
    "            dic[img] = cluster\n",
    "       \n",
    "    if save:\n",
    "        f = open(name, \"a\")\n",
    "        f.write(str(dic))\n",
    "        f.close()\n",
    "    \n",
    "    return dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
