{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rapid-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ghayat/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import visdom\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import shutil\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arranged-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../Resnet/')\n",
    "from model import *\n",
    "from my_ImageFolder import *\n",
    "from fairness_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pending-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bask_r_f = '../Datasets/basket_volley/basket/basket_f_r/'\n",
    "path_bask_y_f = '../Datasets/basket_volley/basket/basket_f_y/'\n",
    "path_bask_r_m = '../Datasets/basket_volley/basket/basket_m_r/'\n",
    "path_bask_y_m = '../Datasets/basket_volley/basket/basket_m_y/'\n",
    "\n",
    "bask_r_f = os.listdir(path_bask_r_f)\n",
    "bask_y_f = os.listdir(path_bask_y_f)\n",
    "bask_r_m = os.listdir(path_bask_r_m)\n",
    "bask_y_m = os.listdir(path_bask_y_m)\n",
    "\n",
    "path_voll_r_f = '../Datasets/basket_volley/volley/volley_f_r/'\n",
    "path_voll_y_f = '../Datasets/basket_volley/volley/volley_f_y/'\n",
    "path_voll_r_m = '../Datasets/basket_volley/volley/volley_m_r/'\n",
    "path_voll_y_m = '../Datasets/basket_volley/volley/volley_m_y/'\n",
    "\n",
    "voll_r_f = os.listdir(path_voll_r_f)\n",
    "voll_y_f = os.listdir(path_voll_y_f)\n",
    "voll_r_m = os.listdir(path_voll_r_m)\n",
    "voll_y_m = os.listdir(path_voll_y_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specialized-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0_min, class1_min = bask_y_m + bask_y_f,voll_r_m + voll_r_f\n",
    "protected_groups = set(class0_min + class1_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "black-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_PROTECTED, BIAS, VAL_MODE, START_EPOCH, NUM_EPOCH, SHOW_PROGRESS, ID, DATASET, NUM_TRIALS = 1, 0.8, False, 0, 10, False, 0, \"basket_volley\", 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "martial-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '../Datasets/basket_volley/train_test_split'\n",
    "image_datasets = {\n",
    "    x: my_ImageFolder(os.path.join(data_dir, f\"train_0.8\" if x == \"train\" else x), data_transforms[x],\n",
    "                      protected_groups, W_PROTECTED)\n",
    "    for x in ['train', 'test']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-matrix",
   "metadata": {},
   "source": [
    "#### Training Conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "young-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 5)\n",
    "        self.fc1 = nn.Linear(33708, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, len(class_names))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crazy-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.fc2.register_forward_hook(get_activation('fc2'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # weighted_cross_entropy_loss\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tropical-schema",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.5731 Acc: 0.7176\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.4885 Acc: 0.7908\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.4165 Acc: 0.8139\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.3525 Acc: 0.8472\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.2766 Acc: 0.8922\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.9114\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.1384 Acc: 0.9499\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.0613 Acc: 0.9833\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.0328 Acc: 0.9936\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.0220 Acc: 1.0000\n",
      "Training complete in 20m 19s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(net, criterion, optimizer, scheduler, dataloaders, dataset_sizes, device,\n",
    "                             start_epoch=START_EPOCH,\n",
    "                             num_epochs=NUM_EPOCH,\n",
    "                             val_mode=VAL_MODE, show_progress=SHOW_PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "front-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. on Training set: 1.0\n",
      "Acc. on Test set: 0.6136919315403423\n",
      "Fairness measures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.94642857, 0.49494949],\n",
       "       [0.13592233, 0.86315789]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Acc. on Training set: {float(accuracy(net, device, dataloaders['train']))}\")\n",
    "print(f\"Acc. on Test set: {float(accuracy(net, device, dataloaders['test']))}\")\n",
    "print(f\"Fairness measures\")\n",
    "demographic_parity(net, device, image_datasets[\"test\"], [class0_min, class1_min]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-convert",
   "metadata": {},
   "source": [
    "#### Training Complete Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complicated-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arbitrary-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss  # nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "secondary-difficulty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.2367 Acc: 0.9692\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 3.7493 Acc: 0.8742\n",
      "Training complete in 3m 14s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, dataloaders, dataset_sizes, device,\n",
    "                             start_epoch=START_EPOCH,\n",
    "                             num_epochs=NUM_EPOCH,\n",
    "                             val_mode=VAL_MODE, show_progress=SHOW_PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bearing-needle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. on Training set: 0.4582798459563543\n",
      "Acc. on Test set: 0.4841075794621027\n",
      "Fairness measures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Acc. on Training set: {float(accuracy(model_conv, device, dataloaders['train']))}\")\n",
    "print(f\"Acc. on Test set: {float(accuracy(model_conv, device, dataloaders['test']))}\")\n",
    "print(f\"Fairness measures\")\n",
    "demographic_parity(model_conv, device, image_datasets[\"test\"], [class0_min, class1_min]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "comic-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repr = nn.Sequential(*list(model_conv.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "classified-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = np.array([[]]).reshape(0, model_conv.fc.in_features)\n",
    "for (inputs, label), weights in dataloaders[\"train\"]:\n",
    "    output = model_repr(inputs).detach().numpy().reshape((inputs.shape[0], -1))\n",
    "    X_transform = np.concatenate([X_transform, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-carter",
   "metadata": {},
   "source": [
    "#### Taking pretrained representation Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "talented-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = np.array([[]]).reshape(0,model_conv.fc.out_features)\n",
    "for (inputs, label), weights in dataloaders[\"train\"]:\n",
    "    output = model_conv(inputs)\n",
    "    X_transform = np.concatenate([X_transform, output])\n",
    "indexes = list(range(len(image_datasets[\"train\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-violin",
   "metadata": {},
   "source": [
    "#### Extracting represenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "negative-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform, labels, indexes = np.array([[]]).reshape(0,net.fc2.out_features), np.array([]).astype(int), np.array([]).astype(int)\n",
    "for i, ((inputs, _), index) in enumerate(dataloaders[\"train\"]):\n",
    "    output = net(inputs)\n",
    "    output = activation['fc2']\n",
    "    X_transform = np.concatenate([X_transform, output])\n",
    "    indexes = np.concatenate([indexes, index.numpy().reshape(-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-envelope",
   "metadata": {},
   "source": [
    "#### PCA reduction with pretrained repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "veterinary-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reducted = PCA(n_components=10).fit_transform(X_transform)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit_predict(X_reducted)\n",
    "path = os.path.join(data_dir, \"train_0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "solid-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_paths = view_clusters(path, kmeans, indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-brass",
   "metadata": {},
   "source": [
    "#### pretrained statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "incorporated-cycle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Cluster 0--------- \n",
      " n. samples: 208\n",
      " n. of bask: 116 (55.8%)\n",
      " n. of volley: 92 (44.2%)\n",
      " n. of red: 106 (51.0%)\n",
      " n. of yellow: 102 (49.0%)\n",
      " n. of males: 106 (51.0%)\n",
      " n. of females: 102 (49.0%)\n",
      "--------------Cluster 1--------- \n",
      " n. samples: 223\n",
      " n. of bask: 130 (58.3%)\n",
      " n. of volley: 93 (41.7%)\n",
      " n. of red: 118 (52.9%)\n",
      " n. of yellow: 105 (47.1%)\n",
      " n. of males: 102 (45.7%)\n",
      " n. of females: 121 (54.3%)\n",
      "--------------Cluster 2--------- \n",
      " n. samples: 185\n",
      " n. of bask: 103 (55.7%)\n",
      " n. of volley: 82 (44.3%)\n",
      " n. of red: 101 (54.6%)\n",
      " n. of yellow: 84 (45.4%)\n",
      " n. of males: 91 (49.2%)\n",
      " n. of females: 94 (50.8%)\n",
      "--------------Cluster 3--------- \n",
      " n. samples: 163\n",
      " n. of bask: 73 (44.8%)\n",
      " n. of volley: 90 (55.2%)\n",
      " n. of red: 84 (51.5%)\n",
      " n. of yellow: 79 (48.5%)\n",
      " n. of males: 68 (41.7%)\n",
      " n. of females: 95 (58.3%)\n"
     ]
    }
   ],
   "source": [
    "statistics(path, cluster_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-treatment",
   "metadata": {},
   "source": [
    "#### Conv statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "corresponding-revision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Cluster 0--------- \n",
      " n. samples: 172\n",
      " n. of bask: 142 (82.6%)\n",
      " n. of volley: 30 (17.4%)\n",
      " n. of red: 128 (74.4%)\n",
      " n. of yellow: 44 (25.6%)\n",
      " n. of males: 81 (47.1%)\n",
      " n. of females: 91 (52.9%)\n",
      "--------------Cluster 1--------- \n",
      " n. samples: 280\n",
      " n. of bask: 280 (100.0%)\n",
      " n. of volley: 0 (0.0%)\n",
      " n. of red: 228 (81.4%)\n",
      " n. of yellow: 52 (18.6%)\n",
      " n. of males: 130 (46.4%)\n",
      " n. of females: 150 (53.6%)\n",
      "--------------Cluster 2--------- \n",
      " n. samples: 238\n",
      " n. of bask: 0 (0.0%)\n",
      " n. of volley: 238 (100.0%)\n",
      " n. of red: 37 (15.5%)\n",
      " n. of yellow: 201 (84.5%)\n",
      " n. of males: 111 (46.6%)\n",
      " n. of females: 127 (53.4%)\n",
      "--------------Cluster 3--------- \n",
      " n. samples: 89\n",
      " n. of bask: 0 (0.0%)\n",
      " n. of volley: 89 (100.0%)\n",
      " n. of red: 16 (18.0%)\n",
      " n. of yellow: 73 (82.0%)\n",
      " n. of males: 45 (50.6%)\n",
      " n. of females: 44 (49.4%)\n"
     ]
    }
   ],
   "source": [
    "statistics(path, cluster_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-stretch",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "optional-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_clusters(path, kmeans, indexes):\n",
    "    K = len(set(kmeans))\n",
    "    bask, voll = os.listdir(os.path.join(path, \"basket\")), os.listdir(os.path.join(path, \"volley\"))\n",
    "    \n",
    "    paths = []\n",
    "    for k in range(K):\n",
    "        paths.append(f\"clustering_{K}/cluster_{k}\")\n",
    "        os.makedirs(paths[-1], exist_ok=True)\n",
    "        \n",
    "    for i in range(len(kmeans)):\n",
    "        src = os.path.join(path, \"basket/\" if indexes[i] < len(bask) else \"volley/\") + (bask[indexes[i]] if indexes[i] < len(bask) else voll[indexes[i] - len(bask)])\n",
    "        dst = f\"clustering_{K}/cluster_{kmeans[i]}/\" + (bask[indexes[i]] if indexes[i] < len(bask) else voll[indexes[i] - len(bask)])\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def statistics(path, clusters):\n",
    "    K = len(set(clusters))\n",
    "    dr, nur = os.listdir(os.path.join(path, \"basket\")), os.listdir(os.path.join(path, \"volley\"))\n",
    "    \n",
    "    for k in range(K):\n",
    "        n_bask, n_voll, n_r, n_y, n_m, n_f = 0, 0, 0, 0, 0, 0\n",
    "        cluster = os.listdir(clusters[k])\n",
    "        for img in cluster:\n",
    "            if img in bask_r_f:\n",
    "                n_bask += 1\n",
    "                n_f += 1\n",
    "                n_r += 1\n",
    "                \n",
    "            if img in bask_r_m:\n",
    "                n_bask += 1\n",
    "                n_m += 1\n",
    "                n_r += 1\n",
    "                \n",
    "            if img in bask_y_f:\n",
    "                n_bask += 1\n",
    "                n_f += 1\n",
    "                n_y += 1\n",
    "            \n",
    "            if img in bask_y_m:\n",
    "                n_bask += 1\n",
    "                n_m += 1\n",
    "                n_y += 1\n",
    "            \n",
    "            if img in voll_r_f:\n",
    "                n_voll += 1\n",
    "                n_f += 1\n",
    "                n_r += 1\n",
    "            \n",
    "            if img in voll_r_m:\n",
    "                n_voll += 1\n",
    "                n_m += 1\n",
    "                n_r += 1\n",
    "                \n",
    "            if img in voll_y_f:\n",
    "                n_voll += 1\n",
    "                n_f += 1\n",
    "                n_y += 1\n",
    "            \n",
    "            if img in voll_y_m:\n",
    "                n_voll += 1\n",
    "                n_m += 1\n",
    "                n_y += 1\n",
    "                \n",
    "        \n",
    "        print(f\"--------------Cluster {k}--------- \\n n. samples: {len(cluster)}\\n n. of bask: {n_bask} ({n_bask/len(cluster)*100:.1f}%)\\n n. of volley: {n_voll} ({n_voll/len(cluster)*100:.1f}%)\\n n. of red: {n_r} ({n_r/len(cluster)*100:.1f}%)\\n n. of yellow: {n_y} ({n_y/len(cluster)*100:.1f}%)\\n n. of males: {n_m} ({n_m/len(cluster)*100:.1f}%)\\n n. of females: {n_f} ({n_f/len(cluster)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
