{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recorded-wales",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "This notebook analyses the resluts of the models previously trained. It considers the three cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "administrative-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absolute-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dr_f_d = '../Datasets/doctor_nurse/dr/fem_dr_dark_56/'\n",
    "path_dr_f_l = '../Datasets/doctor_nurse/dr/fem_dr_light_256/'\n",
    "path_dr_m_d = '../Datasets/doctor_nurse/dr/mal_dr_dark_62/'\n",
    "path_dr_m_l = '../Datasets/doctor_nurse/dr/mal_dr_light_308/'\n",
    "\n",
    "dr_f_d = os.listdir(path_dr_f_d)\n",
    "dr_f_l = os.listdir(path_dr_f_l)\n",
    "dr_m_d = os.listdir(path_dr_m_d)\n",
    "dr_m_l = os.listdir(path_dr_m_l)\n",
    "\n",
    "path_nur_f_d = '../Datasets/doctor_nurse/nurse/fem_nurse_dark_63/'\n",
    "path_nur_f_l = '../Datasets/doctor_nurse/nurse/fem_nurse_light_252/'\n",
    "path_nur_m_d = '../Datasets/doctor_nurse/nurse/mal_nurse_dark_76/'\n",
    "path_nur_m_l = '../Datasets/doctor_nurse/nurse/mal_nurse_light_203/'\n",
    "\n",
    "nur_f_d = os.listdir(path_nur_f_d)\n",
    "nur_f_l = os.listdir(path_nur_f_l)\n",
    "nur_m_d = os.listdir(path_nur_m_d)\n",
    "nur_m_l = os.listdir(path_nur_m_l)\n",
    "\n",
    "dr_m, dr_f = len(dr_m_d) + len(dr_m_l), len(dr_f_d) + len(dr_f_l)\n",
    "\n",
    "w_protected = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-academy",
   "metadata": {},
   "source": [
    "###  Defining dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "progressive-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, K):\n",
    "    \"\"\"Yield successive K-sized chunks from lst.\"\"\"\n",
    "    results, n = [], ceil(len(lst)/K)\n",
    "    for i in range(0, len(lst), n):\n",
    "        results.append(lst[i:(i + n) if i + n < len(lst) else -1])\n",
    "    return results\n",
    "\n",
    "def make_clusters(sets, protected_groups, K):\n",
    "    assert len(sets) == len(protected_groups)\n",
    "    \n",
    "    clusters = []\n",
    "    for i, s in enumerate(sets):\n",
    "        majority, minority = [], []\n",
    "        for img in s:\n",
    "            minority.append(img) if img in protected_groups[i] else majority.append(img)\n",
    "        \n",
    "        clusters.append([minority] + chunks(majority, K-1))\n",
    "        \n",
    "    return clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "applicable-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_ImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform, clusters):\n",
    "        super().__init__(root, transform)\n",
    "        self.clusters = clusters\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        img = self.samples[index][0].split(\"/\")[-1]\n",
    "        group_number = max([[img in c for c in clusters].index(max([img in c for c in clusters])) for clusters in self.clusters])\n",
    "        return super().__getitem__(index), group_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "given-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_dir, BIAS = '../Datasets/doctor_nurse/train_test_split', 0.8\n",
    "image_datasets = {x: my_ImageFolder(os.path.join(data_dir, f\"train_{BIAS}\" if x==\"train\" and BIAS else x), \n",
    "                                    data_transforms[x], \n",
    "                                    make_clusters([os.listdir(os.path.join(data_dir, f\"train_{BIAS}/doctors\" if x==\"train\" and BIAS else x)),\n",
    "                                                    os.listdir(os.path.join(data_dir, f\"train_{BIAS}/nurses\" if x==\"train\" and BIAS else x))],\n",
    "                                                    [set(dr_f_d + dr_f_l), set(nur_m_l + nur_m_d)],\n",
    "                                                      K=5))\n",
    "                  for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-chinese",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "registered-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy_loss(output, labels, weights):\n",
    "    cel = -torch.log(torch.exp(output.gather(1, labels.view(-1,1))) / torch.sum(torch.exp(output), 1).view(-1,1))\n",
    "    weighted_cel = weights * cel.view(-1)\n",
    "    return torch.mean(weighted_cel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-stock",
   "metadata": {},
   "source": [
    "For descirption of architecture of resNet, see: https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bibliographic-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # weighted_cross_entropy_loss\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "north-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_MODE = False\n",
    "EPOCH = 30\n",
    "CASE = \"Case_2\"\n",
    "id = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "super-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH =  f\"{CASE}/checkpoints/\" + (\"w_val\" if VAL_MODE else \"w.o_val\") + f\"/Bias_{BIAS}/model_ep_{EPOCH}/Run_{id}/checkpoint.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model_conv.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_conv.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "exp_lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-arctic",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "beneficial-labor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def accuracy(model, dataloader):\n",
    "    model.eval() \n",
    "    corrects, total = 0, 0\n",
    "    for i, ((inputs, labels), _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        corrects += torch.sum(preds == labels.data)\n",
    "        total += inputs.size(0)\n",
    "        \n",
    "    return corrects.double()/total\n",
    "\n",
    "def demographic_parity(model, test_set):\n",
    "    dr_path = os.path.join(test_set, \"doctors\") \n",
    "    nurs_path = os.path.join(test_set, \"nurses\") \n",
    "    \n",
    "    dr_m_indices, dr_f_indices = split_gender(dr_path, dr_m_l + dr_m_d)\n",
    "    nurs_m_indices, nurs_f_indices = split_gender(nurs_path, nur_m_d + nur_m_l)\n",
    "        \n",
    "    dr_m = torch.utils.data.Subset(image_datasets[\"test\"], indices=dr_m_indices)\n",
    "    dr_f = torch.utils.data.Subset(image_datasets[\"test\"], indices=dr_f_indices)\n",
    "    \n",
    "    nurs_m = torch.utils.data.Subset(image_datasets[\"test\"], indices=[len(dr_m + dr_f) + i for i in nurs_m_indices])\n",
    "    nurs_f = torch.utils.data.Subset(image_datasets[\"test\"], indices=[len(dr_m + dr_f) + i for i in nurs_f_indices])\n",
    "    \n",
    "    dataloaders = [torch.utils.data.DataLoader(x, batch_size=4, shuffle=True, num_workers=4) for x in [dr_m, dr_f, nurs_m, nurs_f]]  \n",
    "    accuracies = [[float(accuracy(model, dataloader)) for dataloader in dataloaders[:2]], [float(accuracy(model, dataloader)) for dataloader in dataloaders[2:]]]\n",
    "    \n",
    "    return pd.DataFrame(accuracies, index=[\"Doctor\", \"Nurse\"], columns=[\"Men\", \"Women\"])\n",
    "    \n",
    "       \n",
    "def split_gender(path, male_group):\n",
    "    s = sorted(os.listdir(path))\n",
    "    \n",
    "    l1, l2 = [], []\n",
    "    for i, image in enumerate(s):\n",
    "        if image in male_group:\n",
    "            l1.append(i)\n",
    "        else:\n",
    "            l2.append(i)\n",
    "            \n",
    "    return l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-consideration",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "assured-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Datasets/doctor_nurse/train_test_split/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-spelling",
   "metadata": {},
   "source": [
    "## BIAS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-intent",
   "metadata": {},
   "source": [
    "### Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-calendar",
   "metadata": {},
   "source": [
    "#### Model after 0 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "generic-charity",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(501, dtype=torch.int32) total: 907\n",
      "tensor(0.5524, dtype=torch.float64)\n",
      "Correct:  tensor(153, dtype=torch.int32) total: 296\n",
      "tensor(0.5169, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-split",
   "metadata": {},
   "source": [
    "#### Model after 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "mounted-friendly",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(758, dtype=torch.int32) total: 907\n",
      "tensor(0.8357, dtype=torch.float64)\n",
      "Correct:  tensor(235, dtype=torch.int32) total: 296\n",
      "tensor(0.7939, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "blessed-guitar",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(235, dtype=torch.int32) total: 296\n",
      "tensor(0.7939, dtype=torch.float64)\n",
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(77, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(60, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(48, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(50, dtype=torch.int32) total: 73\n",
      "[tensor(0.8953, dtype=torch.float64), tensor(0.8219, dtype=torch.float64), tensor(0.7500, dtype=torch.float64), tensor(0.6849, dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "print(demographic_parity(model_conv, path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-motel",
   "metadata": {},
   "source": [
    "#### Model after 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "classified-whole",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(756, dtype=torch.int32) total: 907\n",
      "tensor(0.8335, dtype=torch.float64)\n",
      "Correct:  tensor(238, dtype=torch.int32) total: 296\n",
      "tensor(0.8041, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "certified-contrary",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(68, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(51, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(57, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(62, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.698630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.849315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.790698  0.698630\n",
       "Nurse   0.890625  0.849315"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-square",
   "metadata": {},
   "source": [
    "#### Model after 45 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "endless-voice",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(761, dtype=torch.int32) total: 907\n",
      "tensor(0.8390, dtype=torch.float64)\n",
      "Correct:  tensor(239, dtype=torch.int32) total: 296\n",
      "tensor(0.8074, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "lovely-vertex",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(74, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(57, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(51, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(57, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.860465  0.780822\n",
       "Nurse   0.796875  0.780822"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-campus",
   "metadata": {},
   "source": [
    "#### Model after 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "designed-ability",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(763, dtype=torch.int32) total: 907\n",
      "tensor(0.8412, dtype=torch.float64)\n",
      "Correct:  tensor(243, dtype=torch.int32) total: 296\n",
      "tensor(0.8209, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "seven-leave",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(75, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(58, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(53, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(57, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.794521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.872093  0.794521\n",
       "Nurse   0.828125  0.780822"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-discussion",
   "metadata": {},
   "source": [
    "#### Model after 75 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "western-madonna",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(767, dtype=torch.int32) total: 907\n",
      "tensor(0.8456, dtype=torch.float64)\n",
      "Correct:  tensor(241, dtype=torch.int32) total: 296\n",
      "tensor(0.8142, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "veterinary-belief",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(72, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(54, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(57, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(58, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.794521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.837209  0.739726\n",
       "Nurse   0.890625  0.794521"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-check",
   "metadata": {},
   "source": [
    "### Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "occupied-porter",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(771, dtype=torch.int32) total: 907\n",
      "tensor(0.8501, dtype=torch.float64)\n",
      "Correct:  tensor(239, dtype=torch.int32) total: 296\n",
      "tensor(0.8074, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "becoming-count",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(74, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(53, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(56, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(56, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.726027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.767123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.860465  0.726027\n",
       "Nurse   0.875000  0.767123"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-optimum",
   "metadata": {},
   "source": [
    "## BIAS = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-funds",
   "metadata": {},
   "source": [
    "###  Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-union",
   "metadata": {},
   "source": [
    "#### After 15 epcohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "expanded-brass",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(541, dtype=torch.int32) total: 611\n",
      "tensor(0.8854, dtype=torch.float64)\n",
      "Correct:  tensor(225, dtype=torch.int32) total: 296\n",
      "tensor(0.7601, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fifteen-hampton",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(74, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(50, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(40, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(61, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.684932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.835616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.860465  0.684932\n",
       "Nurse   0.625000  0.835616"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-convergence",
   "metadata": {},
   "source": [
    "#### After 30 epcohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "infrared-spelling",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. training set:  0.8821603927986906\n",
      "ACC test set:  0.7567567567567568\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.876712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.848837  0.630137\n",
       "Nurse   0.640625  0.876712"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Acc. training set: \", float(accuracy(model_conv, dataloaders['train'])))\n",
    "print(\"ACC test set: \", float(accuracy(model_conv, dataloaders['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "convinced-newport",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(73, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(46, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(41, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(64, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.876712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.848837  0.630137\n",
       "Nurse   0.640625  0.876712"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-custom",
   "metadata": {},
   "source": [
    "#### After 45 epcohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bulgarian-hearts",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(546, dtype=torch.int32) total: 611\n",
      "tensor(0.8936, dtype=torch.float64)\n",
      "Correct:  tensor(224, dtype=torch.int32) total: 296\n",
      "tensor(0.7568, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "palestinian-madrid",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(73, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(47, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(41, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(63, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.643836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.863014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.848837  0.643836\n",
       "Nurse   0.640625  0.863014"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-ridge",
   "metadata": {},
   "source": [
    "###  Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-furniture",
   "metadata": {},
   "source": [
    "#### After 15 epcohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "color-aging",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(522, dtype=torch.int32) total: 611\n",
      "tensor(0.8543, dtype=torch.float64)\n",
      "Correct:  tensor(231, dtype=torch.int32) total: 296\n",
      "tensor(0.7804, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "quantitative-collar",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(75, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(53, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(46, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(57, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.726027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.872093  0.726027\n",
       "Nurse   0.718750  0.780822"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-badge",
   "metadata": {},
   "source": [
    "#### After 30 epcohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "municipal-dayton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. training set:  0.8461538461538461\n",
      "ACC test set:  0.8006756756756757\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.821918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.837209  0.739726\n",
       "Nurse   0.796875  0.821918"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Acc. training set: \", float(accuracy(model_conv, dataloaders['train'])))\n",
    "print(\"ACC test set: \", float(accuracy(model_conv, dataloaders['test'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "through-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(72, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(54, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(51, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(60, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.821918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.837209  0.739726\n",
       "Nurse   0.796875  0.821918"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-entry",
   "metadata": {},
   "source": [
    "#### After 45 epcohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "automated-oxygen",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  tensor(521, dtype=torch.int32) total: 611\n",
      "tensor(0.8527, dtype=torch.float64)\n",
      "Correct:  tensor(235, dtype=torch.int32) total: 296\n",
      "tensor(0.7939, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model_conv, dataloaders['train']))\n",
    "print(accuracy(model_conv, dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "foreign-sleeping",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 73 159\n",
      "64 73 137\n",
      "Correct:  tensor(71, dtype=torch.int32) total: 86\n",
      "Correct:  tensor(51, dtype=torch.int32) total: 73\n",
      "Correct:  tensor(51, dtype=torch.int32) total: 64\n",
      "Correct:  tensor(62, dtype=torch.int32) total: 73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.698630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.849315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.825581  0.698630\n",
       "Nurse   0.796875  0.849315"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-platinum",
   "metadata": {},
   "source": [
    "### Case 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-cassette",
   "metadata": {},
   "source": [
    "#### After 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "compound-canyon",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. training set:  0.8870703764320785\n",
      "ACC test set:  0.7567567567567568\n"
     ]
    }
   ],
   "source": [
    "print(\"Acc. training set: \", float(accuracy(model_conv, dataloaders['train'])))\n",
    "print(\"ACC test set: \", float(accuracy(model_conv, dataloaders['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "meaning-latex",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.808219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.848837  0.630137\n",
       "Nurse   0.718750  0.808219"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-marriage",
   "metadata": {},
   "source": [
    "#### After 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "quantitative-flashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. training set:  0.8821603927986906\n",
      "ACC test set:  0.7601351351351351\n"
     ]
    }
   ],
   "source": [
    "print(\"Acc. training set: \", float(accuracy(model_conv, dataloaders['train'])))\n",
    "print(\"ACC test set: \", float(accuracy(model_conv, dataloaders['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "israeli-state",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.616438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.821918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.825581  0.616438\n",
       "Nurse   0.765625  0.821918"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-privilege",
   "metadata": {},
   "source": [
    "#### After 45 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "matched-slovenia",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. training set:  0.8821603927986906\n",
      "ACC test set:  0.7635135135135135\n"
     ]
    }
   ],
   "source": [
    "print(\"Acc. training set: \", float(accuracy(model_conv, dataloaders['train'])))\n",
    "print(\"ACC test set: \", float(accuracy(model_conv, dataloaders['test'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-simpson",
   "metadata": {},
   "source": [
    "#### After 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "informative-stuart",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. training set:  0.8936170212765957\n",
      "ACC test set:  0.7432432432432432\n"
     ]
    }
   ],
   "source": [
    "print(\"Acc. training set: \", float(accuracy(model_conv, dataloaders['train'])))\n",
    "print(\"ACC test set: \", float(accuracy(model_conv, dataloaders['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abandoned-exhaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doctor</th>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.602740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nurse</th>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.849315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Men     Women\n",
       "Doctor  0.837209  0.602740\n",
       "Nurse   0.656250  0.849315"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity(model_conv, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "familiar-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "/home/ghayat/.local/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8569558101472996 += 0.006088731665380755\n",
      "Test accuracy: 0.8006756756756758 += 0.004777748521530725\n",
      "Fairness accuracy: \n",
      " [[0.85116279 0.71780822]\n",
      " [0.81875    0.80821918]] += [[0.01708946 0.02540717]\n",
      " [0.01875    0.0150061 ]]\n"
     ]
    }
   ],
   "source": [
    "VAL_MODE = False\n",
    "EPOCH =30\n",
    "CASE = \"Case_1\"\n",
    "id = 0\n",
    "START_EPOCH = 0\n",
    "NUM_EPOCH = 30\n",
    "BIAS = 0.8\n",
    "\n",
    "train_accs, test_accs, fairness_accs, fairness_diffs = [],[],[],[]\n",
    "for i in range(5):\n",
    "    PATH =  f\"{CASE}/checkpoints/\" + (\"w_val\" if VAL_MODE else \"w.o_val\") + f\"/Bias_{BIAS}/model_ep_{EPOCH}/Run_{id}/trial_{i}/checkpoint.pt\"\n",
    "    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "    model_conv.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer_conv.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    exp_lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    train_accs.append(accuracy(model_conv, dataloaders['train']))\n",
    "    test_accs.append(accuracy(model_conv, dataloaders['test']))\n",
    "    fairness_accs.append(demographic_parity(model_conv, os.path.join(data_dir, \"test\")).to_numpy())\n",
    "    fairness_diffs.append((abs(fairness_accs[-1][\"Doctor\"][\"Men\"] - fairness_accs[-1][\"Doctor\"][\"Women\"]), abs(fairness_accs[-1][\"Nurse\"][\"Men\"] - fairness_accs[-1][\"Nurse\"][\"Women\"])))\n",
    "\n",
    "train_accs = np.array(train_accs)\n",
    "test_accs = np.array(test_accs)\n",
    "fairness_accs = np.array(fairness_accs)\n",
    "\n",
    "\n",
    "print(f\"Training accuracy: {train_accs.mean()} += {train_accs.std()}\")\n",
    "print(f\"Test accuracy: {test_accs.mean()} += {test_accs.std()}\")\n",
    "print(f\"Fairness accuracy: \\n {np.mean(fairness_accs, axis=0)} += {np.std(fairness_accs, axis=0)}\")\n",
    "print(\"Fairness accuracy difference per trial: \", fairness_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "immediate-envelope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Datasets/doctor_nurse/train_test_split/test/doctors/10. male-doctor-portrait-isolated-white-background-56744085.jpg',\n",
       " 0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets[\"test\"].samples[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
